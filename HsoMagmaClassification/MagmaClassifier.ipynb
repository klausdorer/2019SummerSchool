{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://imla.hs-offenburg.de/\"> <img src=\"https://imla.hs-offenburg.de/typo3conf/ext/hsotemplate/Resources/Public/Images/logo.png\" alt=\"Header\" />\n",
    "<img src=\"https://imla.hs-offenburg.de/fileadmin/_processed_/csm_IMLA-Header_7729879e18.png\" width=\"100%\" alt=\"Header\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magma Classification\n",
    "In this domain objects on pictures of our soccer robot Sweaty shall be classified. From eight possible object classes the correct class should be recognized. The eight classes are:\n",
    "  \n",
    " 0 Ball  \n",
    " 1 Goalpost  \n",
    " 2 Obstacle  \n",
    " 3 L-Line  \n",
    " 4 X-Line  \n",
    " 5 T-Line  \n",
    " 6 Penalty Spot  \n",
    " 7 Foot  \n",
    "\n",
    "As at the beginning of every Python script, the Python modules to be used must be made known to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import Adam,Adagrad,Adamax,Adadelta,Nadam,RMSprop,SGD\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing import image as keras_image\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16 , preprocess_input,decode_predictions\n",
    "\n",
    "from os.path import isfile\n",
    "\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# For Jetson TX2!\n",
    "# prevent Tensorflow to use all available memory \n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCH = 4\n",
    "# Keras Optimizer: Adam, Adagrad, Adamax, Adadelta, Nadam, RMSprop, SGD\n",
    "OPTIMIZER = Adam()\n",
    "\n",
    "CNN_MODEL = 'MagmaCnnClassifier.hdf5'\n",
    "\n",
    "DATA_SHAPE=(100, 100, 3)\n",
    "TARGET_SHAPE = 8\n",
    "TRAIN_DIR = \"./data/training\"\n",
    "VAL_DIR = \"./data/validation\"\n",
    "TEST_DIR = \"./data/test\"\n",
    "\n",
    "LOGS_DIR = \"./logs\"\n",
    "\n",
    "prediction_dir = \"./data/prediction\"\n",
    "\n",
    "CATEGORIE_NAMES = ['BALL',]\n",
    "CATEGORIES = ['0','1','2','3','4','5','6','7']\n",
    "TESTING_SIZE = 20 #MAX = SIZE OF IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Augmentation\n",
    "\n",
    "\n",
    "For the Mnist example, Keras provides functions to obtain the data set prepared in Training and Test as well as in X (independent variable (image)) and Y (dependent variable (class/number)) separately. This step must be performed manually for your own data records. However, Keras offers functions to read and prepare the data in this case, so-called generators. The [ImageDataGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class) class exists for the image classification domain. When such an object is initialized, augmentation parameters (*rescale, rotation\\_range,...*) can be defined that manipulate the images accordingly. The flow_from_directory method is used to define the directory and the target size for the images. \n",
    "\n",
    "Keras also determines the class labels from the folder structure of the training images. Images with lable *0* are stored in a folder _./data/train/0_ .\n",
    "You can check this with a code cell  `!ls ./data/training` . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1 / 255,\n",
    "#    rotation_range=20,\n",
    "#    width_shift_range=0.2,\n",
    "#    height_shift_range=0.2,\n",
    "#    shear_range=0.2,\n",
    "#    zoom_range=0.2,\n",
    "#    horizontal_flip=True,\n",
    "# Do train/validation split with a single directory,\n",
    "# depents on 'subset' param in flow_from...()\n",
    "#    validation_split=0.9, \n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1 / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=VAL_DIR,\n",
    "    target_size=DATA_SHAPE[:2],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    target_size=DATA_SHAPE[:2],\n",
    "    batch_size=1,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "After the data set has been prepared for the training, the network structure is defined and the function for _loss_, _optimizer_ and _metrics_ is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=DATA_SHAPE),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dropout(rate=0.5),\n",
    "    Dense(units=512, activation='relu'),\n",
    "    Dense(units=TARGET_SHAPE, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks\n",
    "\n",
    "By using the well-known TensorBoard Callback we get information about the training process again. In addition we use the ModelCheckpoint Callback for the persistent storage of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dir = time.strftime(\"%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "\n",
    "tb_cb = TensorBoard(log_dir=LOGS_DIR+'/'+sub_dir,\n",
    "                    write_graph=True,\n",
    "                    write_images=True,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "\n",
    "mc_cb = ModelCheckpoint(filepath=os.path.join(LOGS_DIR,sub_dir,CNN_MODEL),\n",
    "                        verbose=1,\n",
    "                        save_best_only=True)\n",
    "\n",
    "callbacks = [tb_cb, mc_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Since the training process in this scenario takes place with generators, the corresponding method must also be used ([fit_generator(..)](https://keras.io/models/sequential/#fit_generator)). The parameters to be defined include the generators (*train\\_generator, val\\_generator*) and the number of steps for training and validation (*steps\\_per\\_epoch, validation\\_steps*) for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                              epochs=EPOCH,\n",
    "                              validation_data=val_generator,\n",
    "                              callbacks=callbacks,\n",
    "                              steps_per_epoch=train_generator.n//BATCH_SIZE,\n",
    "                              validation_steps=val_generator.n//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now that the model has been trained, we want to determine how well the model responds to previously unseen data and classifies it. As a metric we use the functions Accuracy and F1-Score, further information is provided by a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and the [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) provided by the Machine Learning Library [scikit-learn](https://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "(test_loss,test_acc) = model.evaluate_generator(test_generator, test_generator.n,  verbose=0)\n",
    "print('TEST_LOSS: {}\\nTEST_ACCURACY: {}'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score, f1_score\n",
    "\n",
    "test_generator.reset()\n",
    "prediction = model.predict_generator(test_generator, steps=test_generator.n, verbose=1)\n",
    "\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(np.squeeze(np.array(prediction)),axis=1)\n",
    "\n",
    "# same score as with model.evaluate(..)?\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_true,y_pred)))\n",
    "print('F1_Score: {:.3f}'.format(f1_score(y_true,y_pred,average='weighted')))\n",
    "display(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of False Negative and False Positive\n",
    "\n",
    "Below is a representation of the *False Negative* and *False Positive* in relation to the class `LABEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 0\n",
    "\n",
    "indices = np.arange(test_generator.n)\n",
    "indices = indices[y_true==LABEL]\n",
    "true_positive_idx = indices[y_pred[indices]==LABEL]\n",
    "false_negative_idx = indices[y_pred[indices]!=LABEL] \n",
    "\n",
    "indices = np.arange(test_generator.n)\n",
    "indices = indices[y_pred==LABEL]\n",
    "false_positive_idx = indices[y_true[indices]!=LABEL]\n",
    "\n",
    "def print_images(indices,suptitel):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    fig.suptitle(suptitel, fontsize=16)\n",
    "    for i,idx in enumerate(indices[:10]):\n",
    "        (img,y) = test_generator[idx]\n",
    "        filename = test_generator.filenames[idx]\n",
    "        img = np.squeeze(img)\n",
    "\n",
    "        ax = plt.subplot(2,5,i+1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title('{}\\nPrediction: {}'.format(filename,y_pred[idx]))\n",
    "        ax.tick_params(bottom=False, left=False,  labelleft=False, labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.8)\n",
    "\n",
    "print_images(false_negative_idx, 'False Negative')\n",
    "print_images(false_positive_idx, 'False Positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights into the Model \n",
    "\n",
    "After we have dealt extensively with the training of convolutional networks, we want to allow ourselves a little insight into the inside of such a black box. One of the simplest visualizations is that of the Feature Maps, which is done first. Using the heat map representation, a representation can be generated that describes which aspect of an image is important for the final classification result of the model.\n",
    "\n",
    "The examples are taken from the source [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) and adapted to the model accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess simple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use either the false_positive or false negative images for visualization\n",
    "if len(false_positive_idx)!=0: sub_set = false_positive_idx\n",
    "elif len(false_negative_idx) !=0: sub_set = false_negative_idx\n",
    "else: raise Exception('NO ERROR: false_positive_idx and false_negative_idx ')\n",
    "\n",
    "rand_idx = random.randint(0,len(sub_set)-1)\n",
    "(img,label) = test_generator[sub_set[rand_idx]]\n",
    "filename = test_generator.filenames[sub_set[rand_idx]]\n",
    "\n",
    "image2predict = img.copy()\n",
    "\n",
    "img = np.squeeze(img)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show each layer channels (8 Top layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "\n",
    "activation_model = keras.models.Model(inputs=model.input,outputs=layer_outputs)\n",
    "\n",
    "activations = activation_model.predict(image2predict)\n",
    "\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "    \n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names,activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row *size)).astype(np.uint8)\n",
    "    \n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,:,:,col*images_per_row+row]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image,0,255).astype('uint8')\n",
    "            display_grid[col*size : (col+1)*size , row*size : (row+1)*size] = channel_image\n",
    "\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale*display_grid.shape[1],scale*display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid,aspect='auto',cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sample picture\n",
    "plt.imshow(np.squeeze(img))\n",
    "plt.title(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(image2predict)\n",
    "pred = np.round(pred, 3)\n",
    "result = np.argmax(pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model.output[:,result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = model.get_layer('conv2d_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(model_output,last_layer.output)[0]\n",
    "\n",
    "pooled_grads = K.mean(grads, axis = (0,1,2))\n",
    "\n",
    "iterate = K.function([model.input],[pooled_grads, last_layer.output[0]])\n",
    "\n",
    "pooled_grads_value , conv_layer_output_value = iterate([image2predict])\n",
    "\n",
    "i=0\n",
    "while i<128:\n",
    "    conv_layer_output_value[:,:,i]  *= pooled_grads_value[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_cv = cv2.imread(os.path.join(TEST_DIR,filename))\n",
    "\n",
    "heatmap = np.mean(conv_layer_output_value,axis=-1)\n",
    "heatmap = np.maximum(heatmap,0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.imshow(heatmap)\n",
    "\n",
    "heatmap = cv2.resize(heatmap,(img_cv.shape[1],img_cv.shape[0]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
    "\n",
    "#heatmap = cv2.cvtColor(heatmap,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superimposed_image = heatmap * 0.4 + img_cv\n",
    "# opencv conversions \n",
    "cv2.imwrite('superimposed_image.png', superimposed_image)\n",
    "plt.imshow(plt.imread('superimposed_image.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnet Filter Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "    x += 0.5\n",
    "    x = np.clip(x,0,1)\n",
    "    x *= 255\n",
    "    x = np.clip(x,0,255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pattern(layer_name,filter_index,size = 150):\n",
    "    layer_output = model.get_layer(layer_name).output\n",
    "    loss = K.mean(layer_output[:,:,:,filter_index])\n",
    "    grads = K.gradients(loss,model.input)[0]\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "    iterate = K.function([model.input],[loss,grads])\n",
    "    \n",
    "    input_img_data = np.random.random((1,size,size,3)) * 20 + 128\n",
    "    \n",
    "    step = 1\n",
    "    for i in range(40):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "    \n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "progress_info = display.DisplayHandle()\n",
    "progress_info.display('')\n",
    "\n",
    "LAYER_NAME = 'conv2d_4'\n",
    "channels_count = model.get_layer(LAYER_NAME).output.shape[-1].value\n",
    "\n",
    "w = DATA_SHAPE[0]\n",
    "h = DATA_SHAPE[1]\n",
    "margin = 10\n",
    "COLS = 8\n",
    "rows = channels_count//COLS\n",
    "\n",
    "result = np.zeros((h * rows + (rows-1)*margin,w * COLS + (COLS-1)*margin,3),dtype=np.uint8)\n",
    "plt.figure(figsize=(25,35))\n",
    "\n",
    "for i in range(channels_count):\n",
    "    img = generate_pattern(LAYER_NAME,i,w)\n",
    "    \n",
    "    col = i % COLS\n",
    "    row = i // COLS\n",
    "    \n",
    "    from_x, to_x = col*w+margin*col,(col*w+w)+margin*col\n",
    "    from_y, to_y = row*h+margin*row,(row*w+w)+margin*row\n",
    "    #print(row,col)\n",
    "    #print('IMAGE_POS: X:',from_x,':', to_x,'Y:',from_y,':',to_y  )\n",
    "    result[from_y:to_y,from_x:to_x] = img\n",
    "    progress_info.update('PROCESSED CHANNELS: {} of {}'.format(i+1,channels_count))\n",
    "    \n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
