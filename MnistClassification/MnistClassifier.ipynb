{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://imla.hs-offenburg.de/\"> \n",
    "![hs logo](../images/hs-logo.png)  \n",
    "![IMLA Banner](../images/imla-banner.png)\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Summer School of the Hochschule Offenburg! Before we start, here are a few **notes** about Jupyter notebooks.\n",
    "\n",
    "1. This Jupyter notebook is rendered in your browser, but the content is streamed by an interactive iPython kernel.\n",
    "\n",
    "2. The notebook consists of cells; cells can contain code that you can run, or they can contain text and/or images that you can read.\n",
    "\n",
    "3. You can execute code cells by clicking on the ```Run``` icon in the menu or by using the following key combinations ```Shift-Enter``` (execute and forward) or ```Ctrl-Enter``` (execute and stay in the current cell).\n",
    "\n",
    "4. To interrupt cell execution, click the ```Stop``` button on the toolbar or navigate to the ```Kernel``` menu and select ```Interrupt Kernel```.\n",
    "\n",
    "5. The button ```H``` gives you an overview of the configured keyboard shortcuts and information about the modes (Command Mode and Edit Mode) of Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am a text cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm a code cell.\n",
    "1 + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a prefixed '!' it is possible to execute Bash commands within code cells as well\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a Python library that allows you to programmatically create and train artificial neural networks. Keras is an abstraction layer to existing Deep Learning Frameworks (Tensorflow, CNTK and Theano). It therefore uses the mentioned frameworks as backend.  \n",
    "  \n",
    "In this notebook we want to deal with the classification of the Mnist dataset, the Hello World of Deep Learning, and thus slowly familiarize ourselves with the Keras Framework. As at the beginning of every Python script, the Python modules to be used must be made known to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential,load_model, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import metrics as k_metrics\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing import image as keras_image\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# For Jetson TX2!\n",
    "# prevent Tensorflow to use all available memory \n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "In the following, constants are defined, which determine the further execution. In addition, some hyperparameters are determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DENSE_MODEL = 'MnistDenseClassifier.hdf5'\n",
    "CNN_MODEL = 'MnistCnnClassifier.hdf5'\n",
    "IMAGE_DIR = './data'\n",
    "\n",
    "TARGET_SHAPE = (28, 28, 1)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "INVERSE_DATA = False # Don't use b/w inverse images\n",
    "MODEL_TYPE = CNN_MODEL\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER = RMSprop()\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%#\n"
    }
   },
   "source": [
    "## Data Preparation\n",
    "Now the images are loaded and normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if MODEL_TYPE is CNN_MODEL:\n",
    "    x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "    x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "elif MODEL_TYPE is DENSE_MODEL:\n",
    "    TARGET_SHAPE = (784,)\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "if INVERSE_DATA:\n",
    "    # invert the data (black/white) and concatenate it to the existing date.\n",
    "    x_train = np.concatenate((x_train, np.invert(x_train)))\n",
    "    y_train = np.concatenate((y_train,y_train))\n",
    "    x_test = np.concatenate((x_test, np.invert(x_test)))\n",
    "    y_test = np.concatenate((y_test, y_test))\n",
    "\n",
    "# normalize the data\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Now the network is created depending on **MODEL_TYPE**. The _Sequential_ class shows that in both cases it is a simple _Feedforward Network_. After the model definition, the _compile_ method adds functions for _loss_, _optimizer_ and _metrics_ to the model for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if MODEL_TYPE is DENSE_MODEL:\n",
    "    model = Sequential([\n",
    "        Dense(units=512, activation='relu', input_shape=TARGET_SHAPE),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "if MODEL_TYPE is CNN_MODEL:\n",
    "    model = Sequential([\n",
    "            Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=TARGET_SHAPE),\n",
    "            MaxPooling2D(pool_size=(3, 3)),\n",
    "            Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2, 2)),\n",
    "            Flatten(),\n",
    "            Dense(units=32, activation='relu'),\n",
    "            Dense(units=NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# compile the model. \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=[k_metrics.categorical_accuracy])\n",
    "\n",
    "# print the models layout\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%#\n"
    }
   },
   "source": [
    "## Model Training\n",
    "\n",
    "Now we have the training data and a model ready. Before we do the training (_fit(..)_), we create a tensorboard callback, which logs the training process. We can view the progress by starting Tensorboard with `tensorboard --logdir=/path/to/logdir/`. After the successful start of Tensorboard it is accessible via port 6006 via the browser `localhost:6006`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sub_dir = time.strftime(\"%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "tb_cb = TensorBoard(log_dir='./logs/'+sub_dir,\n",
    "                    #histogram_freq=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    write_grads=True,\n",
    "                    write_images=True,\n",
    "                    #update_freq='batch',\n",
    "                   )\n",
    "callbacks = [tb_cb]\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.16666,             \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation  \n",
    "\n",
    "Now that we have a trained model, we want to determine its quality. The test data set is available in the variables x_train (images) and y_train (classes). With the method *evaluate(..)* we can finally determine the accuracy of the model.\n",
    "\n",
    "\n",
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_loss,test_acc) = model.evaluate(x_test,y_test, verbose=0)\n",
    "print('TEST_LOSS: {:.3f}\\nTEST_ACCURACY: {:.3f}'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix & Classification Report\n",
    "The Accuracy and the F1_Score were calculated again using the Machine Learning library [scikit-learn](https://scikit-learn.org/stable/index.html) as a proof for the model evaluation. In addition, the library offers methods to create a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)  and [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score, f1_score\n",
    "\n",
    "# make a prediction for the complete test set\n",
    "prediction = model.predict(x_test, verbose=1)\n",
    "\n",
    "y_true = np.argmax(np.squeeze(np.array(y_test)),axis=1)\n",
    "# transform prediction to class label (for each image the class)\n",
    "y_pred = np.argmax(np.squeeze(np.array(prediction)),axis=1)\n",
    "\n",
    "# same score as with model.evaluate(..)?\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_true,y_pred)))\n",
    "print('F1_Score: {:.3f}'.format(f1_score(y_true,y_pred,average='weighted')))\n",
    "display(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of False Negative and False Positive\n",
    "\n",
    "Below is a representation of the *False Negative* and *False Positive* in reference to the class `LABEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 0\n",
    "\n",
    "indices = np.arange(len(y_true))\n",
    "indices = indices[y_true==LABEL]\n",
    "true_positive_idx = indices[y_pred[indices]==LABEL]\n",
    "false_negative_idx = indices[y_pred[indices]!=LABEL] \n",
    "\n",
    "indices = np.arange(len(y_true))\n",
    "indices = indices[y_pred==LABEL]\n",
    "false_positive_idx = indices[y_true[indices]!=LABEL]\n",
    "\n",
    "def print_images(indices,suptitel):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    fig.suptitle(suptitel, fontsize=16)\n",
    "    for i,idx in enumerate(indices[:20]):\n",
    "        img = x_test[idx]\n",
    "        img = np.reshape(img,(28,28))\n",
    "        ax = plt.subplot(2,10,i+1)\n",
    "        ax.imshow(img,cmap='gray')\n",
    "        ax.set_title('True Class: {}\\nPrediction: {}\\nProp:{:.3f}'.format(y_true[idx],\n",
    "                                                                          y_pred[idx],\n",
    "                                                                          prediction[idx][y_pred[idx]] ))\n",
    "        ax.tick_params(bottom=False, left=False,  labelleft=False, labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.8)\n",
    "\n",
    "print_images(false_negative_idx, 'False Negative')\n",
    "print_images(false_positive_idx, 'False Positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of own Numbers\n",
    "\n",
    "Now that we have adapted the model to the training data, we want to test some of our own pictures. For this purpose there are predefined images under the directory `./data/`, which are loaded one after the other and submitted to the network for classification. The result image presents the individual results of each image. For each image the result shows the maximum value of the Softmax function and the predicted class (number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images = glob.glob(IMAGE_DIR + '/*.jpg')\n",
    "images.sort()\n",
    "\n",
    "COLS = 8\n",
    "rows = len(images)//COLS+1\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "for idx, image in enumerate(images):\n",
    "    \n",
    "    ax = plt.subplot(rows,COLS,idx+1)\n",
    "    \n",
    "    img = keras_image.load_img(path=image, color_mode='grayscale', target_size=(28, 28, 1))\n",
    "    img = keras_image.img_to_array(img)\n",
    "    img2predict = img.copy()\n",
    "    \n",
    "    if MODEL_TYPE is DENSE_MODEL:\n",
    "        img2predict = np.reshape(img,TARGET_SHAPE)\n",
    "    img2predict = np.expand_dims(img2predict,0)\n",
    "    img2predict /= 255\n",
    "    \n",
    "    pred = model.predict(img2predict)\n",
    "    pred = np.ravel(pred)\n",
    "    pred = np.round(pred, 3)\n",
    "    \n",
    "    class_predict = np.argmax(pred)\n",
    "    # show the output of the model\n",
    "    #print(image, pred)\n",
    "    img = np.squeeze(img)\n",
    "    ax.imshow(img,cmap='gray')\n",
    "    ax.set_title('{}\\n Prob: {:0.3f}\\n Prediction: {}'.format(image,pred[class_predict],class_predict ))\n",
    "    ax.tick_params(bottom=False, left=False,  labelleft=False, labelbottom=False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
